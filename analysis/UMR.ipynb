{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7dd1cc2-d891-47a0-b879-cee75de8a34b",
   "metadata": {},
   "source": [
    "# UMR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a1935e-6a8e-4d95-89cd-b44368b035e8",
   "metadata": {},
   "source": [
    "## 导入模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8adf70ef-9af5-4a0d-95c1-db3e05dc31b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import feather\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import sunlandsdatasdk as sd\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee95977f-7fdd-43c6-9a30-a06eed8e65fd",
   "metadata": {},
   "source": [
    "## 读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a40a263-46e3-409b-9742-946bb4ee1a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.to_datetime('2023-01-01')\n",
    "end_date = pd.to_datetime('2023-12-31')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732e56d6-6e67-44f8-95f2-dc0bcf1222c0",
   "metadata": {},
   "source": [
    "### 市值 (用于计算流通股本)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ccabcbc-6219-48a3-99c9-bcfe7bc2f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sd.auth('intern', 'a8cc57a3812802722e7c7a5cd92c2140')\n",
    "\n",
    "# issues = sd.get_index_stocks('999998', date=end_date)\n",
    "# market_cap = sd.get_ricequant_factor(issues, 20230101, 20231231, ['market_cap', 'market_cap_2'])\n",
    "# market_cap = market_cap.reset_index()\n",
    "# market_cap['date'] = pd.to_datetime(market_cap['date'])\n",
    "\n",
    "# free_ratio = market_cap['market_cap_2'].div(market_cap['market_cap']).mean()\n",
    "# market_cap['market_cap_2'] = market_cap['market_cap_2'].fillna(market_cap['market_cap'] * free_ratio)\n",
    "\n",
    "# feather.write_dataframe(market_cap, '../data/market_cap.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5230d8af-f8ee-4cbc-945a-fbb0a1b282bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "market_cap = feather.read_dataframe('../data/market_cap.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93af1554-0b45-4cba-ac14-dd6893c87dc5",
   "metadata": {},
   "source": [
    "### 日线数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69631ecf-582c-48d7-bcec-336b5a7d0d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_1d = feather.read_dataframe('../data/StockPriceK1d_20241231.feather')\n",
    "price_1d = price_1d[(price_1d['date'] >= start_date) & (price_1d['date'] <= end_date)]\n",
    "price_1d = pd.merge(\n",
    "    price_1d,\n",
    "    market_cap,\n",
    "    on=['date', 'issue'],\n",
    "    how='left'\n",
    ")\n",
    "price_1d['free_float'] = price_1d['market_cap_2'].div(price_1d['close'])\n",
    "price_1d = price_1d.set_index('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b734e996-ebca-4eb0-9822-0f6cd409cdef",
   "metadata": {},
   "source": [
    "### 指数数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69c8cbc4-6716-49a5-97b3-80d4096fc40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs500 = feather.read_dataframe('../data/IndexPriceK1m_000905.feather')\n",
    "hs500['ret_index'] = hs500['close'] / hs500['close'].shift(1) - 1\n",
    "\n",
    "hs500['date'] = pd.to_datetime(hs500['date'], format='ISO8601')\n",
    "hs500 = hs500[(hs500['date'] >= start_date) & (hs500['date'] <= end_date)]\n",
    "hs500 = hs500.set_index('date')\n",
    "\n",
    "trade_time = hs500['time']\n",
    "map_trade_time = {t: t - 100 for t in trade_time}\n",
    "map_trade_time[100000] = 95900\n",
    "map_trade_time[110000] = 105900\n",
    "map_trade_time[140000] = 135900\n",
    "map_trade_time[150000] = 145900\n",
    "hs500['time'] = hs500['time'].apply(map_trade_time.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e7fefd-e298-4679-bad4-32fcfa3eef14",
   "metadata": {},
   "source": [
    "### 分钟线测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcb0f7cd-90ad-4f2f-b3b1-cd1713c5bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_1m_fields = ['date', 'time', 'issue', 'high', 'low', 'close']\n",
    "price_1d_fields = ['preclose', 'free_float']\n",
    "def price_1m_read(date:np.datetime64):\n",
    "    year = date.year\n",
    "    date_str = date.strftime('%Y%m%d')\n",
    "    price_1m = feather.read_dataframe(f'../data/StockPriceK1m/{year}/StockPriceK1m_{date_str}.feather')\n",
    "    price_1m['date'] = pd.to_datetime(price_1m['date'], format='ISO8601')\n",
    "    \n",
    "    price_1m = pd.merge(\n",
    "        price_1m[price_1m_fields],\n",
    "        price_1d.loc[date, ['issue'] + price_1d_fields],\n",
    "        on='issue',\n",
    "        how='left'\n",
    "    )\n",
    "    price_1m['close_prev'] = (\n",
    "        price_1m\n",
    "            .groupby('issue')['close']\n",
    "            .shift(1).fillna(price_1m['preclose'])\n",
    "    )\n",
    "    price_1m['ret'] = price_1m['close'] / price_1m['close_prev'] - 1\n",
    "    \n",
    "    price_1m = pd.merge(\n",
    "        price_1m,\n",
    "        hs500.loc[date, ['time', 'ret_index']],\n",
    "        on='time',\n",
    "        how='left'\n",
    "    )\n",
    "    price_1m['excess'] = price_1m['ret'] - price_1m['ret_index']\n",
    "    # price_1m['excess'] = price_1m['ret']\n",
    "    \n",
    "    return price_1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9824958f-6fae-44ad-ad2b-96d89a3ed606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_1m_read_fast(date: pd.Timestamp) -> pd.DataFrame:\n",
    "    year = date.year\n",
    "    date_str = date.strftime('%Y%m%d')\n",
    "\n",
    "    price_1m = feather.read_dataframe(f'../data/StockPriceK1m/{year}/StockPriceK1m_{date_str}.feather')\n",
    "    price_1m['date'] = pd.to_datetime(price_1m['date'], format='ISO8601')\n",
    "    price_1m = price_1m[price_1m_fields]\n",
    "\n",
    "    # 减少后续分组/映射的开销（按你的真实数据类型选择）\n",
    "    # 若 issue 是代码（数字或字符串不太多），用 category\n",
    "    if price_1m['issue'].dtype != 'category':\n",
    "        price_1m['issue'] = price_1m['issue'].astype('category')\n",
    "\n",
    "    # ===== 用 map 代替 merge: preclose by issue =====\n",
    "    # 期望 price_1d.loc[date, ['issue','preclose']] 返回当日一张表\n",
    "    preclose_df = price_1d.loc[date, ['issue'] + price_1d_fields]\n",
    "    # 建立以 issue 为索引的 Series（类型与 price_1m['issue'] 对齐）\n",
    "    preclose_s = preclose_df.set_index('issue')[price_1d_fields]\n",
    "    # 若 issue 是分类，确保映射索引类型一致\n",
    "    if isinstance(price_1m['issue'].dtype, pd.CategoricalDtype):\n",
    "        preclose_s.index = preclose_s.index.astype(price_1m['issue'].dtype.categories.dtype)\n",
    "\n",
    "    for field in price_1d_fields:\n",
    "        price_1m[field] = price_1m['issue'].map(preclose_s[field])\n",
    "\n",
    "    # ===== 组内上一分钟收盘 =====\n",
    "    # 避免排序带来的额外成本；确保原始数据已经按时间顺序\n",
    "    # 若不确定，请先按 ['issue','time'] 排序一次（只在必要时）：\n",
    "    # price_1m.sort_values(['issue','time'], inplace=True, kind='mergesort')\n",
    "    close_prev = price_1m.groupby('issue', sort=False, observed=False)['close'].shift(1)\n",
    "    # 首分钟用 preclose 补\n",
    "    price_1m['close_prev'] = close_prev.where(close_prev.notna(), price_1m['preclose'])\n",
    "\n",
    "    # 收益率\n",
    "    # ret = close/prev - 1（完全矢量化）\n",
    "    price_1m['ret'] = price_1m['close'].div(price_1m['close_prev']).sub(1.0)\n",
    "\n",
    "    # ===== 用 map 代替 merge: index return by time =====\n",
    "    # 期望 hs500.loc[date, ['time','ret_index']] 返回当日分钟级指数收益\n",
    "    idx_df = hs500.loc[date, ['time', 'ret_index']]\n",
    "    ret_index_s = idx_df.set_index('time')['ret_index']\n",
    "\n",
    "    price_1m['ret_index'] = price_1m['time'].map(ret_index_s)\n",
    "\n",
    "    # 超额收益\n",
    "    price_1m['excess'] = price_1m['ret'].sub(price_1m['ret_index'])\n",
    "\n",
    "    return price_1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08e29f7c-c82c-45c8-838e-f2a1851be455",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = pd.to_datetime('2023-01-03')\n",
    "price_1m = price_1m_read_fast(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8274e5d4-237c-43f4-a320-885fbae66cb5",
   "metadata": {},
   "source": [
    "## 计算风险指标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf061d-687e-4b16-b631-5fe33e77d1b8",
   "metadata": {},
   "source": [
    "### 计算调整后风险系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fef1a34-a52e-4450-b2e9-65e5a590fb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_calc(r:pd.Series, d:int=10):\n",
    "    return r.rolling(d, min_periods=1).mean() - r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b381956c-7ad7-4f98-9fd2-84fd62212048",
   "metadata": {},
   "source": [
    "### 真实波动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "684d45e3-03a8-416d-9d00-2692051c9afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_34380\\1002163620.py:9: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  tr['risk'] = tr.groupby('issue')['r'].transform(risk_calc)\n"
     ]
    }
   ],
   "source": [
    "def tr_calc(price_1m:pd.DataFrame):\n",
    "    tr = price_1m.copy()\n",
    "    tr['tr1'] = tr['high'] - tr['low']\n",
    "    tr['tr2'] = np.abs(tr['high'] - tr['close_prev'])\n",
    "    tr['tr3'] = np.abs(tr['low'] - tr['close_prev'])\n",
    "    tr['r'] = tr[['tr1', 'tr2', 'tr3']].max(axis=1) / tr['close_prev']\n",
    "    return tr[['date', 'time', 'issue', 'r']]\n",
    "tr = tr_calc(price_1m)\n",
    "tr['risk'] = tr.groupby('issue')['r'].transform(risk_calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7349c3-4314-4c8e-8937-61a5df78ceca",
   "metadata": {},
   "source": [
    "### 换手率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930721b8-d578-425b-a59b-242d2538df65",
   "metadata": {},
   "source": [
    "## 计算动量反转因子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beea9f5-f2e5-45eb-a612-9bfeb2c2a002",
   "metadata": {},
   "source": [
    "### 计算单日内反转因子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eb01b0a-374f-4102-a056-9ec8c8d6d74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rev_calc(date:np.datetime64, price_1m:pd.DataFrame, risk:pd.DataFrame=None, m:int=60, decay:bool=True):\n",
    "    m = 60\n",
    "    H = m / 2\n",
    "    weight = 2 ** ((np.arange(m) - m) / H)\n",
    "    weight = weight / weight.sum()\n",
    "\n",
    "    ret_risk = price_1m[['time', 'issue', 'excess']].copy()\n",
    "    if risk is None:\n",
    "        ret_risk['risk'] = -1\n",
    "    else:\n",
    "        ret_risk = pd.merge(\n",
    "            ret_risk,\n",
    "            risk[['time', 'issue', 'risk']],\n",
    "            on=['time', 'issue'],\n",
    "            how='left'\n",
    "        ).sort_values(['issue', 'time'])\n",
    "    weight = np.tile(weight, len(ret_risk['issue'].unique()))\n",
    "    \n",
    "    start_time = np.array([93000, 103000, 130000, 135400])\n",
    "    end_time = np.array([102900, 112900, 135900, 145300])\n",
    "    rev = None\n",
    "    for st, et in zip(start_time, end_time):\n",
    "        rr = ret_risk[(ret_risk['time'] >= st) & (ret_risk['time'] <= et)].copy()\n",
    "        if not decay:\n",
    "            rr['weight'] = 1\n",
    "        else:\n",
    "            rr['weight'] = weight\n",
    "\n",
    "        rr['rev'] = rr['weight'] * rr['risk'] * rr['excess']\n",
    "        rev_time = (\n",
    "            rr\n",
    "                .groupby('issue')['rev']\n",
    "                .sum().reset_index()\n",
    "        )\n",
    "        rev_time['time'] = et\n",
    "        rev = pd.concat([rev, rev_time])\n",
    "    rev['date'] = date\n",
    "    rev = rev.sort_values(['issue', 'time']).reset_index(drop=True)\n",
    "    return rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "839c17f7-fd02-48c2-a586-a90af24eaab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rev_calc_fast(date: np.datetime64,\n",
    "                  price_1m: pd.DataFrame,\n",
    "                  risk: pd.DataFrame | None = None,\n",
    "                  m: int = 60,\n",
    "                  decay: bool = True) -> pd.DataFrame:\n",
    "    # ===== 1) 预处理与dtype =====\n",
    "    # 只取用到的列，避免多余拷贝\n",
    "    df = price_1m.loc[:, ['time', 'issue', 'excess']].copy()\n",
    "\n",
    "    # 更紧凑的dtype：加速比较/分组；issue考虑转为分类\n",
    "    if not isinstance(df['issue'].dtype, pd.CategoricalDtype):\n",
    "        df['issue'] = df['issue'].astype('category')\n",
    "\n",
    "    # ===== 2) left join：避免排序、避免copy =====\n",
    "    if risk is None:\n",
    "        # 直接常量列，避免后续merge\n",
    "        df['risk'] = -1.0\n",
    "    else:\n",
    "        # 同样做列与dtype瘦身\n",
    "        r = risk.loc[:, ['time', 'issue', 'risk']].copy()\n",
    "        if not isinstance(r['issue'].dtype, pd.CategoricalDtype):\n",
    "            # 与左表对齐分类，避免合并后再重编码\n",
    "            r['issue'] = r['issue'].astype(df['issue'].dtype)\n",
    "        # merge 不排序（sort=False），并避免多余拷贝（copy=False）\n",
    "        df = df.merge(r, on=['time', 'issue'], how='left', sort=False, copy=False)\n",
    "\n",
    "    # ===== 3) 一次排序 + 权重映射 =====\n",
    "    # 只排序一次（稳定归并排序）。后面不再排序全表。\n",
    "    df.sort_values(['issue', 'time'], kind='mergesort', inplace=True, ignore_index=True)\n",
    "\n",
    "    if decay:\n",
    "        H = m / 2\n",
    "        w = 2.0 ** ((np.arange(m) - m) / H)\n",
    "        w /= w.sum()\n",
    "        w = np.tile(w, len(df['issue'].unique()))\n",
    "    else:\n",
    "        w = 1.0\n",
    "\n",
    "    # ===== 4) 在4个时间窗内聚合：用 numpy bincount 替代 groupby.sum =====\n",
    "    start_time = np.array([93000, 103000, 130000, 135400], dtype=np.int32)\n",
    "    end_time   = np.array([102900, 112900, 135900, 145300], dtype=np.int32)\n",
    "\n",
    "    out_frames = []\n",
    "    # 预取列为ndarray，避免反复取Series开销\n",
    "    t_vals = df['time'].to_numpy()\n",
    "    issue_vals = df['issue'].to_numpy()\n",
    "    # 将分类直接转codes，后续bincount更快\n",
    "    if isinstance(df['issue'].dtype, pd.CategoricalDtype):\n",
    "        issue_codes = df['issue'].cat.codes.to_numpy()\n",
    "        issue_uniques = df['issue'].cat.categories\n",
    "    else:\n",
    "        # 回退：factorize\n",
    "        issue_codes, uniques = pd.factorize(issue_vals, sort=False)\n",
    "        issue_uniques = pd.Index(uniques)\n",
    "\n",
    "    risk_vals = df['risk'].to_numpy(dtype='float64')\n",
    "    exc_vals = df['excess'].to_numpy(dtype='float64')\n",
    "\n",
    "    # 逐窗mask + bincount聚合\n",
    "    for st, et in zip(start_time, end_time):\n",
    "        mask = (t_vals >= st) & (t_vals <= et)\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        codes_sub = issue_codes[mask]\n",
    "        rev_prod = (w * risk_vals[mask] * exc_vals[mask])\n",
    "\n",
    "        # bincount按类别求和，速度远快于 groupby('issue').sum()\n",
    "        # minlength 保证完整长度，避免回表错位\n",
    "        sums = np.bincount(codes_sub, weights=rev_prod, minlength=len(issue_uniques))\n",
    "\n",
    "        # 只取在此窗出现过的issue（提升下游concat效率）\n",
    "        present = np.unique(codes_sub)\n",
    "        rev_time = pd.DataFrame({\n",
    "            'issue': issue_uniques.take(present),\n",
    "            'rev':   sums[present]\n",
    "        })\n",
    "        rev_time['time'] = np.int32(et)\n",
    "        out_frames.append(rev_time)\n",
    "\n",
    "    # ===== 5) 拼接 + 标日期 + 仅对输出排序 =====\n",
    "    rev = pd.concat(out_frames, ignore_index=True) if out_frames else \\\n",
    "          pd.DataFrame(columns=['issue', 'rev', 'time'])\n",
    "    rev['date'] = date\n",
    "    rev.sort_values(['issue', 'time'], kind='mergesort', inplace=True, ignore_index=True)\n",
    "    return rev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a823e357-6cd3-492c-977f-123f9c0b7465",
   "metadata": {},
   "source": [
    "### 遍历所有交易日"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56676f1f-293d-4e1b-adb7-039480d436ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trade_date = price_1d.index.sort_values().unique()\n",
    "# label = 'tr_240m'\n",
    "# risk_func = tr_calc\n",
    "# risk_prev = None\n",
    "# os.makedirs(f'../data/factor_rev/{label}_rev/', exist_ok=True)\n",
    "# for date in tqdm(trade_date):\n",
    "#     year = date.year\n",
    "#     date_str = date.strftime('%Y%m%d')\n",
    "#     os.makedirs(f'../data/factor_rev/{label}_rev/{year}/', exist_ok=True)\n",
    "#     price_1m = price_1m_read_fast(date)\n",
    "    \n",
    "#     risk = risk_func(price_1m)\n",
    "#     risk_2d = pd.concat([risk_prev, risk])\n",
    "#     risk_prev = risk\n",
    "#     risk_2d['risk'] = risk_2d.groupby('issue', observed=False)['r'].transform(risk_calc, d=240)\n",
    "#     risk = risk_2d[risk_2d['date'] == date]\n",
    "    \n",
    "#     rev = rev_calc_fast(date, price_1m, risk=risk, decay=True)\n",
    "#     feather.write_dataframe(rev, f'../data/factor_rev/{label}_rev/{year}/{label}_rev_{date_str}.feather')\n",
    "#     del price_1m, risk, rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de6ea2d1-966a-437b-9f17-7be3b56592a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def datetime_calc(date:pd.Series, time:pd.Series):\n",
    "#     hh = time // 10000\n",
    "#     mm = (time % 10000) // 100\n",
    "#     ss = time % 100\n",
    "#     timedelta = pd.to_timedelta(hh, 'h') + pd.to_timedelta(mm, 'm') + pd.to_timedelta(ss, 's')\n",
    "#     datetime = date + timedelta\n",
    "#     return datetime\n",
    "\n",
    "# rev = None\n",
    "# for date in trade_date:\n",
    "#     year = date.year\n",
    "#     date_str = date.strftime('%Y%m%d')\n",
    "#     rev_daily = feather.read_dataframe(f'../data/factor_rev/{label}_rev/{year}/{label}_rev_{date_str}.feather')\n",
    "#     rev = pd.concat([rev, rev_daily])\n",
    "# rev['datetime'] = datetime_calc(rev['date'], rev['time'])\n",
    "# rev = rev.reset_index(drop=True)\n",
    "# feather.write_dataframe(rev, f'../data/factor_rev/{label}_rev/{label}_rev.feather')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
